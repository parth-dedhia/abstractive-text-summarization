{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>headlines</th>\n",
       "      <th>read_more</th>\n",
       "      <th>text</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chhavi Tyagi</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>http://www.hindustantimes.com/india-news/raksh...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daisy Mowke</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>http://www.hindustantimes.com/bollywood/malaik...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arshiya Chopra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Virgin' now corrected to 'Unmarried' in IGIMS'...</td>\n",
       "      <td>http://www.hindustantimes.com/patna/bihar-igim...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sumedha Sehra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/abu-dujana-...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aarushi Maheshwari</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/sex-traffic...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sonu Kumari</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Man found dead at Delhi police station, kin al...</td>\n",
       "      <td>http://www.hindustantimes.com/delhi-news/man-f...</td>\n",
       "      <td>A 32-year-old man on Wednesday was found hangi...</td>\n",
       "      <td>An alleged suspect in a kidnapping case was fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Parmeet Kaur</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Delhi HC reduces aid for 'negligent' accident ...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/delhi-high-...</td>\n",
       "      <td>The Delhi High Court reduced the compensation ...</td>\n",
       "      <td>In an interesting ruling, the Delhi high court...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Chhavi Tyagi</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>60-yr-old lynched over rumours she was cutting...</td>\n",
       "      <td>http://www.hindustantimes.com/india-news/60-ye...</td>\n",
       "      <td>A 60-year old Dalit woman was allegedly lynche...</td>\n",
       "      <td>A 60-year old Dalit woman was allegedly lynch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Parmeet Kaur</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Chopper flying critically low led to 2015 Bomb...</td>\n",
       "      <td>http://www.hindustantimes.com/mumbai-news/2015...</td>\n",
       "      <td>An inquiry by the Aircraft Accident Investigat...</td>\n",
       "      <td>Two years after a helicopter crash near the Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sumedha Sehra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Congress opens 'State Bank of Tomato' in Lucknow</td>\n",
       "      <td>http://indiatoday.intoday.in/story/lucknow-sta...</td>\n",
       "      <td>The Congress party has opened a bank called 'S...</td>\n",
       "      <td>It sounds like satire, but make no mistake: at...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                  date  \\\n",
       "0        Chhavi Tyagi  03 Aug 2017,Thursday   \n",
       "1         Daisy Mowke  03 Aug 2017,Thursday   \n",
       "2      Arshiya Chopra  03 Aug 2017,Thursday   \n",
       "3       Sumedha Sehra  03 Aug 2017,Thursday   \n",
       "4  Aarushi Maheshwari  03 Aug 2017,Thursday   \n",
       "5         Sonu Kumari  03 Aug 2017,Thursday   \n",
       "6        Parmeet Kaur  03 Aug 2017,Thursday   \n",
       "7        Chhavi Tyagi  03 Aug 2017,Thursday   \n",
       "8        Parmeet Kaur  03 Aug 2017,Thursday   \n",
       "9       Sumedha Sehra  03 Aug 2017,Thursday   \n",
       "\n",
       "                                           headlines  \\\n",
       "0  Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1  Malaika slams user who trolled her for 'divorc...   \n",
       "2  Virgin' now corrected to 'Unmarried' in IGIMS'...   \n",
       "3  Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4  Hotel staff to get training to spot signs of s...   \n",
       "5  Man found dead at Delhi police station, kin al...   \n",
       "6  Delhi HC reduces aid for 'negligent' accident ...   \n",
       "7  60-yr-old lynched over rumours she was cutting...   \n",
       "8  Chopper flying critically low led to 2015 Bomb...   \n",
       "9   Congress opens 'State Bank of Tomato' in Lucknow   \n",
       "\n",
       "                                           read_more  \\\n",
       "0  http://www.hindustantimes.com/india-news/raksh...   \n",
       "1  http://www.hindustantimes.com/bollywood/malaik...   \n",
       "2  http://www.hindustantimes.com/patna/bihar-igim...   \n",
       "3  http://indiatoday.intoday.in/story/abu-dujana-...   \n",
       "4  http://indiatoday.intoday.in/story/sex-traffic...   \n",
       "5  http://www.hindustantimes.com/delhi-news/man-f...   \n",
       "6  http://indiatoday.intoday.in/story/delhi-high-...   \n",
       "7  http://www.hindustantimes.com/india-news/60-ye...   \n",
       "8  http://www.hindustantimes.com/mumbai-news/2015...   \n",
       "9  http://indiatoday.intoday.in/story/lucknow-sta...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "5  A 32-year-old man on Wednesday was found hangi...   \n",
       "6  The Delhi High Court reduced the compensation ...   \n",
       "7  A 60-year old Dalit woman was allegedly lynche...   \n",
       "8  An inquiry by the Aircraft Accident Investigat...   \n",
       "9  The Congress party has opened a bank called 'S...   \n",
       "\n",
       "                                               ctext  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  \n",
       "5  An alleged suspect in a kidnapping case was fo...  \n",
       "6  In an interesting ruling, the Delhi high court...  \n",
       "7   A 60-year old Dalit woman was allegedly lynch...  \n",
       "8  Two years after a helicopter crash near the Bo...  \n",
       "9  It sounds like satire, but make no mistake: at...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('news_summary.csv')#,encoding=\"ISO-8859-1\")\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(how='any',axis=0) \n",
    "x = dataset['ctext']\n",
    "y = dataset['text']\n",
    "# x.datatype\n",
    "# print(x[0])\n",
    "t=[]\n",
    "max_length_target,max_length_input = max(x),max(y)\n",
    "# print(len(x[0]))\n",
    "# for i in range(len(x)):\n",
    "#     for j in range(len(x[i])):\n",
    "#         t.append(x[j].split(\".\"))\n",
    "# print(t)\n",
    "# list_of_words=[]\n",
    "# t.append(x[0].split(\".\"))\n",
    "# list_of_words.append(t[0][0].split(\" \"))\n",
    "# print(list_of_words)\n",
    "\n",
    "# print(len(t[0]))\n",
    "# for i in range(len(t[0])):\n",
    "#     t.append(t[0][i])\n",
    "# print(t)\n",
    "\n",
    "\n",
    "# s= pd.Series([x[0],np.nan])\n",
    "# print(s)\n",
    "# li =[]\n",
    "# s.str.split(pat=\".\")\n",
    "# print(s[0])\n",
    "# for i in x[0]:\n",
    "#     c=0\n",
    "#     if(i != '.'):\n",
    "#         c+=1\n",
    "#     else:\n",
    "#         li.append()\n",
    "        \n",
    "headline = dataset['headlines']\n",
    "hardik = dataset['headlines']\n",
    "hardik_text = dataset['text']\n",
    "# print(headline)\n",
    "del dataset\n",
    "# headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> '+ w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  \n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape : (4395, 5791) Target Tensor Shape : (4395, 86)\n",
      "Preprocessed Text : \n",
      " <start> the daman and diu administration on wednesday withdrew a circular that asked women staff to tie rakhis on male colleagues after the order triggered a backlash from employees and was ripped apart on social media . the union territory ? s administration was forced to retreat within hours of issuing the circular that made it compulsory for its staff to celebrate rakshabandhan at workplace . ? it has been decided to celebrate the festival of rakshabandhan on august . in this connection , all offices departments shall remain open and celebrate the festival collectively at a suitable time wherein all the lady staff shall tie rakhis to their colleagues , ? the order , issued on august by gurpreet singh , deputy secretary personnel , had said . to ensure that no one skipped office , an attendance report was to be sent to the government the next evening . the two notifications ? one mandating the celebration of rakshabandhan left and the other withdrawing the mandate right ? were issued by the daman and diu administration a day apart . the circular was withdrawn through a one line order issued late in the evening by the ut ? s department of personnel and administrative reforms . ? the circular is ridiculous . there are sensitivities involved . how can the government dictate who i should tie rakhi to ? we should maintain the professionalism of a workplace ? an official told hindustan times earlier in the day . she refused to be identified . the notice was issued on daman and diu administrator and former gujarat home minister praful kodabhai patel ? s direction , sources said . rakshabandhan , a celebration of the bond between brothers and sisters , is one of several hindu festivities and rituals that are no longer confined of private , family affairs but have become tools to push politic al ideologies . in , the year bjp stormed to power at the centre , rashtriya swayamsevak sangh rss chief mohan bhagwat said the festival had ? national significance ? and should be celebrated widely ? to protect hindu culture and live by the values enshrined in it ? . the rss is the ideological parent of the ruling bjp . last year , women ministers in the modi government went to the border areas to celebrate the festival with soldiers . a year before , all cabinet ministers were asked to go to their constituencies for the festival . <end>\n",
      "Tokenized Text INP : \n",
      " [   42     1 11198 ...     0     0     0]\n",
      "Tokenized Text OUT : \n",
      " [    9     1   920     7   283  2138  8029     8  8030    13  3702    57\n",
      "   298    15   114    21  2139    14   116     4  3347  8031     4    54\n",
      "  1101  3703    11     1  2023     7  6420    11   630     2     1   920\n",
      "    17   659     4  2291     1   423   331   346     7  2292     1  3348\n",
      "    27    21   303 10963    26   536     8    17   372    11   321   229\n",
      "     2    10     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "Headline Text : \n",
      " <start> daman diu revokes mandatory rakshabandhan in offices order <end>\n",
      "Headline Tokenized : \n",
      " [   1 2721 2722 2723  631 2724    4 1597  632    2    0    0    0    0\n",
      "    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# f=0\n",
    "x = [preprocess_sentence(w) for w in x]\n",
    "y = [preprocess_sentence(w) for w in y]\n",
    "headline = [preprocess_sentence(w) for w in headline]\n",
    "x_train, inp_lang = tokenize(x)\n",
    "y_train, targ_lang = tokenize(y)\n",
    "headline_train , headline_lang = tokenize(headline)\n",
    "print(\"Input Tensor Shape :\" , x_train.shape , \"Target Tensor Shape :\" , y_train.shape)\n",
    "print(\"Preprocessed Text : \\n\" , x[0])\n",
    "# embeddings = embed(x[0])\n",
    "print(\"Tokenized Text INP : \\n\" , x_train[0])\n",
    "print(\"Tokenized Text OUT : \\n\" , y_train[0])\n",
    "\n",
    "print(\"Headline Text : \\n\" , headline[0])\n",
    "print(\"Headline Tokenized : \\n\" , headline_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove vctor loaded\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION= 50\n",
    "# data_directory = \"/glove\"\n",
    "# glove_weights_file_path = \"/glove/glove.6B.50d.txt\"\n",
    "\n",
    "# PAD_TOKEN = 0\n",
    "word2idx= [] \n",
    "idx2word= []\n",
    "weights = []\n",
    "# f = open(\"glove_weights_file_path\",\"r\")\n",
    "counts =[]\n",
    "count =0\n",
    "with open('glove.6B.50d.txt',encoding='utf-8') as file: \n",
    "    for index, line in enumerate(file): \n",
    "        values = line.split() # Word and weights separated by space \n",
    "        word = (values[0]) # Word is first symbol on each line \n",
    "        word_weights = np.asarray(values[1:], dtype=np.float32) # Remainder of line is weights for word \n",
    "        word2idx.append(word) # PAD is our zeroth index so shift by one \n",
    "        weights.append(word_weights)\n",
    "        counts.append(index)\n",
    "n_vec = index+1\n",
    "# n_vec = i + 1\n",
    "hidden_dim = len(line.split(' ')) - 1\n",
    "\n",
    "vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
    "\n",
    "with open('glove.6B.50d.txt', encoding = 'utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "\n",
    "average_vec = np.mean(vecs, axis=0) #unknown vector\n",
    "print(\"glove vctor loaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      "[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n"
     ]
    }
   ],
   "source": [
    "# weights.insert(0, np.random.randn(EMBEDDING_DIMENSION))\n",
    "# print(vecs)\n",
    "print(word2idx[1])\n",
    "print(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12920076 -0.28866628 -0.01224866 -0.05676644 -0.20210965 -0.08389011\n",
      "  0.33359843  0.16045167  0.03867431  0.17833012  0.04696583 -0.00285802\n",
      "  0.29099807  0.04613704 -0.20923874 -0.06613114 -0.06822549  0.07665912\n",
      "  0.3134014   0.17848536 -0.1225775  -0.09916984 -0.07495987  0.06413227\n",
      "  0.14441176  0.60894334  0.17463093  0.05335403 -0.01273871  0.03474107\n",
      " -0.8123879  -0.04688699  0.20193407  0.2031118  -0.03935686  0.06967544\n",
      " -0.01553638 -0.03405238 -0.06528071  0.12250231  0.13991883 -0.17446303\n",
      " -0.08011883  0.0849521  -0.01041659 -0.13705009  0.20127155  0.10069408\n",
      "  0.00653003  0.01685157]\n"
     ]
    }
   ],
   "source": [
    "#Uploading UNKNOWN vectors weights\n",
    "# UNK_TOKEN = len(weights)\n",
    "\n",
    "word2idx.append('UNK')\n",
    "weights.append(average_vec)\n",
    "print(weights[word2idx.index('UNK')])\n",
    "# print(np.random.randn(EMBEDDING_DIMENSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_list =[]\n",
    "# for i in range(7,len(headline[0])-5):\n",
    "    \n",
    "#    h_list.append(headline[0][i])\n",
    "# # h_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 50)\n",
      "(1, 16, 50)\n"
     ]
    }
   ],
   "source": [
    "# Headline Embedding\n",
    "h_features ={}\n",
    "h_list=[]\n",
    "h_features['headline_word_indices'] = nltk.word_tokenize(headline[1])\n",
    "for word in h_features['headline_word_indices']:\n",
    "#     print(h_features['headline_word_indices'])\n",
    "# print(len(h_features['headline_word_indices']))\n",
    "    if word in word2idx:\n",
    "        h_features['headline_word_indices']=weights[word2idx.index(word)]\n",
    "    else:\n",
    "        h_features['headline_word_indices']=weights[word2idx.index('UNK')]\n",
    "    h_list.append(h_features['headline_word_indices'])\n",
    "#     print(h_features['headline_word_indices'])\n",
    "# print(h_list)\n",
    "\n",
    "# h_features =[]\n",
    "# h_features = nltk.word_tokenize(headline[0])\n",
    "# for word in h_features:\n",
    "# #     print(h_features['headline_word_indices'])\n",
    "# # print(len(h_features['headline_word_indices']))\n",
    "#     if word in word2idx:\n",
    "#         h_features.append([weights[word2idx.index(word)]])\n",
    "#     else:\n",
    "#         h_features.append([weights[word2idx.index('UNK')]])\n",
    "# #     print(h_features['headline_word_indices'])\n",
    "\n",
    "# print(len(h_features))\n",
    "# print()\n",
    "temp = tf.convert_to_tensor(h_list)\n",
    "print(temp.shape)\n",
    "temp = tf.reshape(temp,(-1,temp.shape[0],temp.shape[1]))\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 460, 50)\n"
     ]
    }
   ],
   "source": [
    "#document Embedding\n",
    "features = {}\n",
    "d_list=[]\n",
    "features['word_indices'] = nltk.word_tokenize(x[1]) #embedding on the document\n",
    "# features['word_indices'] = [weights[word2idx.index(word)] for word in features['word_indices']]\n",
    "for word in features['word_indices']:\n",
    "    if word in word2idx:\n",
    "        features['word_indices'] = [weights[word2idx.index(word)]]\n",
    "    else:\n",
    "        features['word_indices'] =[weights[word2idx.index('UNK')]]\n",
    "    d_list.append(features['word_indices'])\n",
    "# print(d_list)\n",
    "\n",
    "temp1 = tf.convert_to_tensor(d_list)\n",
    "temp1 = tf.reshape(temp1,(1,temp1.shape[0],temp1.shape[2]))\n",
    "print(temp1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    The code of encoder is used for text summarization. The model needs an initial_state which is the headline of the article.\n",
    "    The input is supposed to be padded sequence for initial state as well as for the input. The model cuurently implements\n",
    "    GRU for the encoder sequence as there is some bug with using lstm with bidirectional. Bidirectional was also tried with \n",
    "    GRU but same bug occured.\n",
    "    \n",
    "    The initialization is based on docuemnt context vector paper referenced from \n",
    "    https://arxiv.org/pdf/1807.08000.pdf paper named \n",
    "    Abstractive and Extractive Text Summarization using DocumentContext Vector and Recurrent Neural Networks.\n",
    "    \n",
    "    However the logic is modeified. The model takes the preprocessed tokenized padded sequence as input for initials state\n",
    "    and the then it is passed through the embedded vector and the size of the output vector after embedding is (batch_size , time , embedding size)\n",
    "    after this the model is averaged about the time axis and gives the output as (batch_size , embedding size).\n",
    "    This tensor is then passed as input to dence vector and is used to give the output size of (batch_zie , lgur_hidden_units)\n",
    "    '''\n",
    "    def __init__(self,units = 16,activation='tanh',recurrent_activation='sigmoid',return_state=True,return_sequence=True , vocab_size=400000 , embedding_dim=50 , batch_size=2):\n",
    "        super (Encoder , self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.return_state = return_state\n",
    "        self.return_sequence = return_sequence\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size,embedding_dim)\n",
    "#         self.lstm = tf.keras.layers.LSTM(units=self.units , activation=self.activation , recurrent_activation=self.activation, return_sequences=self.return_sequence , return_state=self.return_state)\n",
    "        self.gru = tf.keras.layers.GRU(units=self.units , activation=self.activation , recurrent_activation=self.activation, return_sequences=self.return_sequence , return_state=self.return_state)\n",
    "#         self.bidirectional = tf.keras.layers.Bidirectional(self.gru)\n",
    "        self.dense = tf.keras.layers.Dense(units=self.units , input_shape = (self.embedding_dim,))\n",
    "#         print(self.gru)\n",
    "#         print(self.dense)\n",
    "    \n",
    "    def call(self,x,headline):\n",
    "#         x = self.embedding(x)\n",
    "#         headline = self.embedding(headline)\n",
    "        print(x.shape,headline.shape)\n",
    "        headline = tf.reduce_mean(headline,axis=1)\n",
    "        headline  = self.dense(headline)\n",
    "        output , state = self.gru(x,initial_state=headline)\n",
    "#         print(headline)\n",
    "        return output , state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 460, 50) (1, 16, 50)\n",
      "(1, 460, 16)\n",
      "(1, 16)\n"
     ]
    }
   ],
   "source": [
    "# encoder(x_train[:2] , headline_train[:2])\n",
    "output, state = encoder(temp1 ,temp)\n",
    "print(output.shape)\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "#         print(\"Score is :\",score)\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16)\n",
      "(1, 460, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(0)\n",
    "attention_result, attention_weights = attention_layer(state, output)\n",
    "print(attention_result.shape)\n",
    "print(attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units = 16,activation='tanh',recurrent_activation='sigmoid',return_state=True,return_sequence=True , vocab_size=400000 , embedding_dim=50 , batch_size=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.return_state = return_state\n",
    "        self.return_sequence = return_sequence\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "#         self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "#                                        return_sequences=True,\n",
    "#                                        return_state=True,\n",
    "#                                        recurrent_initializer='glorot_uniform')\n",
    "        self.gru = tf.keras.layers.GRU(units=self.units , activation=self.activation , recurrent_activation=self.activation, return_sequences=self.return_sequence , return_state=self.return_state)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, hidden, output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "#         x = self.embedding(x)\n",
    "    \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "#         print(x.shape,context_vector.shape)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "#         print(x.shape,hidden.shape,output.shape)\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[1]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (16, 400000)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder()\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((1, 1, 50)), state, output)\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(features['word_indices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "#     attention_plot = np.zeros((max_length_target, max_length_input))\n",
    "\n",
    "#     sentence = x[0]\n",
    "\n",
    "#     inputs = [x[0].word_index[i] for i in sentence.split(' ')]\n",
    "#     inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "#                                                            maxlen=max_length_input,\n",
    "#                                                            padding='post')\n",
    "    inputs = tf.convert_to_tensor(temp1)\n",
    "    h = tf.convert_to_tensor(temp)\n",
    "    result = ''\n",
    "\n",
    "#     hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, h)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims(tf.expand_dims(tf.convert_to_tensor(weights[word2idx.index('start')]), 0),0)\n",
    "#     print(dec_input.shape)\n",
    "    for t in range(100):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        # storing the attention weights to plot later on\n",
    "#         attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "#         attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "#         print(predicted_id)\n",
    "        result += word2idx[predicted_id] + ' '\n",
    "\n",
    "        if word2idx[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims(tf.expand_dims(tf.convert_to_tensor(weights[predicted_id]), 0),0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate():\n",
    "    result = evaluate()\n",
    "\n",
    "#     print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "#     attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "#     plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 460, 50) (1, 16, 50)\n",
      "Predicted translation: finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay finlay \n"
     ]
    }
   ],
   "source": [
    "translate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx.index('start')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
