{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>headlines</th>\n",
       "      <th>read_more</th>\n",
       "      <th>text</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chhavi Tyagi</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>http://www.hindustantimes.com/india-news/raksh...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daisy Mowke</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>http://www.hindustantimes.com/bollywood/malaik...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arshiya Chopra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Virgin' now corrected to 'Unmarried' in IGIMS'...</td>\n",
       "      <td>http://www.hindustantimes.com/patna/bihar-igim...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sumedha Sehra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/abu-dujana-...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aarushi Maheshwari</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/sex-traffic...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                  date  \\\n",
       "0        Chhavi Tyagi  03 Aug 2017,Thursday   \n",
       "1         Daisy Mowke  03 Aug 2017,Thursday   \n",
       "2      Arshiya Chopra  03 Aug 2017,Thursday   \n",
       "3       Sumedha Sehra  03 Aug 2017,Thursday   \n",
       "4  Aarushi Maheshwari  03 Aug 2017,Thursday   \n",
       "\n",
       "                                           headlines  \\\n",
       "0  Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1  Malaika slams user who trolled her for 'divorc...   \n",
       "2  Virgin' now corrected to 'Unmarried' in IGIMS'...   \n",
       "3  Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4  Hotel staff to get training to spot signs of s...   \n",
       "\n",
       "                                           read_more  \\\n",
       "0  http://www.hindustantimes.com/india-news/raksh...   \n",
       "1  http://www.hindustantimes.com/bollywood/malaik...   \n",
       "2  http://www.hindustantimes.com/patna/bihar-igim...   \n",
       "3  http://indiatoday.intoday.in/story/abu-dujana-...   \n",
       "4  http://indiatoday.intoday.in/story/sex-traffic...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "\n",
       "                                               ctext  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('news_summary.csv')#,encoding=\"ISO-8859-1\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(how='any',axis=0) \n",
    "x = dataset['ctext']\n",
    "y = dataset['text']\n",
    "headline = dataset['headlines']\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  \n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed Text : \n",
      " <start> the food safety and standards authority of india fssai is in the process of creating a network of food banking partners to collect and distribute leftover food from large parties and weddings to the hungry . a notification to create a separate category of food business operators fbos , who will be licensed to deal only with leftover food , has been drafted to ensure the quality of food . ? we are looking at partnering with ngos or organisations that collect , store and distribute surplus food to ensure they maintain certain hygiene and health standards when handling food , ? said pawan agarwal , ceo of fssai . ? tonnes of food is wasted annually . we are looking at creating a mechanism through which food can be collected from restaurants , weddings , large scale parties , ? says pawan agarwal , ? all food , whether it is paid for or distributed free , must meet the country ? s food safety and hygiene standards , ? he said . the organisations in the business of collecting leftover food will now have to work in collaboration with fssai so their efforts can be scaled up . ? tonnes of food is wasted annually and can be used to feed several thousands . we are looking at creating a mechanism through which food can be collected from restaurants , weddings , large scale parties etc , ? said agarwal . the initiative will set up a helpline network where organisations can call in for collection but reaching individuals who want to directly donate food will take time . ? we will have a central helpline number . reaching people at the household level may not be feasible initially but it is an integral part of the long term plan , ? he said . ? we have begun collecting names of people working in the sector . there are still a few months to go before the scheme materialises , ? said agarwal . ? collecting food going waste to feed the hungry is a noble thought but to transport , store and maintain the cold chain of cooked food is a huge challenge . the logistics are a nightmare , which is why we don ? t handle leftovers and only distribute uncooked food that can be cooked locally , ? said kuldip nar , founder of delhi ncr food bank , which has been feeding the poor in cities since . <end>\n",
      "Headline Text : \n",
      " <start> food regulator planning leftover banks to feed hungry people <end>\n"
     ]
    }
   ],
   "source": [
    "x = [preprocess_sentence(w) for w in x]\n",
    "y = [preprocess_sentence(w) for w in y]\n",
    "headline = [preprocess_sentence(w) for w in headline]\n",
    "# x_train, inp_lang = tokenize(x)\n",
    "# y_train, targ_lang = tokenize(y)\n",
    "# headline_train , headline_lang = tokenize(headline)\n",
    "# print(\"Input Tensor Shape :\" , x_train.shape , \"Target Tensor Shape :\" , y_train.shape)\n",
    "print(\"Preprocessed Text : \\n\" , x[10])\n",
    "# print(\"Tokenized Text : \\n\" , x_train[10])\n",
    "print(\"Headline Text : \\n\" , headline[10])\n",
    "# print(\"Headline Tokenized : \\n\" , headline_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove vctor loaded\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIMENSION= 50\n",
    "# data_directory = \"/glove\"\n",
    "# glove_weights_file_path = \"/glove/glove.6B.50d.txt\"\n",
    "\n",
    "# PAD_TOKEN = 0\n",
    "word2idx= [] \n",
    "idx2word= []\n",
    "weights = []\n",
    "# f = open(\"glove_weights_file_path\",\"r\")\n",
    "counts =[]\n",
    "count =0\n",
    "with open('glove.6B.50d.txt',encoding='utf-8') as file: \n",
    "    for index, line in enumerate(file): \n",
    "        values = line.split() # Word and weights separated by space \n",
    "        word = (values[0]) # Word is first symbol on each line \n",
    "        word_weights = np.asarray(values[1:], dtype=np.float32) # Remainder of line is weights for word \n",
    "        word2idx.append(word) # PAD is our zeroth index so shift by one \n",
    "        weights.append(word_weights)\n",
    "        counts.append(index)\n",
    "n_vec = index+1\n",
    "# n_vec = i + 1\n",
    "hidden_dim = len(line.split(' ')) - 1\n",
    "\n",
    "vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
    "\n",
    "with open('glove.6B.50d.txt', encoding = 'utf-8') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "\n",
    "average_vec = np.mean(vecs, axis=0) #unknown vector\n",
    "print(\"glove vctor loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "[ 4.1800e-01  2.4968e-01 -4.1242e-01  1.2170e-01  3.4527e-01 -4.4457e-02\n",
      " -4.9688e-01 -1.7862e-01 -6.6023e-04 -6.5660e-01  2.7843e-01 -1.4767e-01\n",
      " -5.5677e-01  1.4658e-01 -9.5095e-03  1.1658e-02  1.0204e-01 -1.2792e-01\n",
      " -8.4430e-01 -1.2181e-01 -1.6801e-02 -3.3279e-01 -1.5520e-01 -2.3131e-01\n",
      " -1.9181e-01 -1.8823e+00 -7.6746e-01  9.9051e-02 -4.2125e-01 -1.9526e-01\n",
      "  4.0071e+00 -1.8594e-01 -5.2287e-01 -3.1681e-01  5.9213e-04  7.4449e-03\n",
      "  1.7778e-01 -1.5897e-01  1.2041e-02 -5.4223e-02 -2.9871e-01 -1.5749e-01\n",
      " -3.4758e-01 -4.5637e-02 -4.4251e-01  1.8785e-01  2.7849e-03 -1.8411e-01\n",
      " -1.1514e-01 -7.8581e-01]\n"
     ]
    }
   ],
   "source": [
    "print(word2idx[0])\n",
    "print(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12920076 -0.28866628 -0.01224866 -0.05676644 -0.20210965 -0.08389011\n",
      "  0.33359843  0.16045167  0.03867431  0.17833012  0.04696583 -0.00285802\n",
      "  0.29099807  0.04613704 -0.20923874 -0.06613114 -0.06822549  0.07665912\n",
      "  0.3134014   0.17848536 -0.1225775  -0.09916984 -0.07495987  0.06413227\n",
      "  0.14441176  0.60894334  0.17463093  0.05335403 -0.01273871  0.03474107\n",
      " -0.8123879  -0.04688699  0.20193407  0.2031118  -0.03935686  0.06967544\n",
      " -0.01553638 -0.03405238 -0.06528071  0.12250231  0.13991883 -0.17446303\n",
      " -0.08011883  0.0849521  -0.01041659 -0.13705009  0.20127155  0.10069408\n",
      "  0.00653003  0.01685157]\n"
     ]
    }
   ],
   "source": [
    "word2idx.append('UNK')\n",
    "weights.append(average_vec)\n",
    "print(weights[word2idx.index('UNK')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16, 50)\n"
     ]
    }
   ],
   "source": [
    "# Headline Embedding\n",
    "h_features ={}\n",
    "h_list=[]\n",
    "h_features['headline_word_indices'] = nltk.word_tokenize(headline[1])\n",
    "for word in h_features['headline_word_indices']:\n",
    "    if word in word2idx:\n",
    "        h_features['headline_word_indices']=weights[word2idx.index(word)]\n",
    "    else:\n",
    "        h_features['headline_word_indices']=weights[word2idx.index('UNK')]\n",
    "    h_list.append(h_features['headline_word_indices'])\n",
    "\n",
    "h_list = tf.convert_to_tensor(h_list)\n",
    "h_list = tf.reshape(h_list,(-1,h_list.shape[0],h_list.shape[1]))\n",
    "print(h_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 460, 50)\n"
     ]
    }
   ],
   "source": [
    "#document Embedding\n",
    "features = {}\n",
    "d_list=[]\n",
    "features['word_indices'] = nltk.word_tokenize(x[1]) #embedding on the document\n",
    "# features['word_indices'] = [weights[word2idx.index(word)] for word in features['word_indices']]\n",
    "for word in features['word_indices']:\n",
    "    if word in word2idx:\n",
    "        features['word_indices'] = [weights[word2idx.index(word)]]\n",
    "    else:\n",
    "        features['word_indices'] =[weights[word2idx.index('UNK')]]\n",
    "    d_list.append(features['word_indices'])\n",
    "\n",
    "d_list = tf.convert_to_tensor(d_list)\n",
    "d_list = tf.reshape(d_list,(1,d_list.shape[0],d_list.shape[2]))\n",
    "print(d_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    The code of encoder is used for text summarization. The model needs an initial_state which is the headline of the article.\n",
    "    The input is supposed to be padded sequence for initial state as well as for the input. The model cuurently implements\n",
    "    GRU for the encoder sequence as there is some bug with using lstm with bidirectional. Bidirectional was also tried with \n",
    "    GRU but same bug occured.\n",
    "    \n",
    "    The initialization is based on docuemnt context vector paper referenced from \n",
    "    https://arxiv.org/pdf/1807.08000.pdf paper named \n",
    "    Abstractive and Extractive Text Summarization using DocumentContext Vector and Recurrent Neural Networks.\n",
    "    \n",
    "    However the logic is modeified. The model takes the preprocessed tokenized padded sequence as input for initials state\n",
    "    and the then it is passed through the embedded vector and the size of the output vector after embedding is (batch_size , time , embedding size)\n",
    "    after this the model is averaged about the time axis and gives the output as (batch_size , embedding size).\n",
    "    This tensor is then passed as input to dence vector and is used to give the output size of (batch_zie , lgur_hidden_units)\n",
    "    '''\n",
    "    def __init__(self,units = 16,activation='tanh',recurrent_activation='sigmoid',return_state=True,return_sequence=True , vocab_size=30000 , embedding_dim=30 , batch_size=2):\n",
    "        super (Encoder , self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.return_state = return_state\n",
    "        self.return_sequence = return_sequence\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = tf.keras.layers.LSTM(units=self.units , activation=self.activation , recurrent_activation=self.activation, return_sequences=self.return_sequence , return_state=self.return_state)\n",
    "        self.gru = tf.keras.layers.GRU(units=self.units , activation=self.activation , recurrent_activation=self.activation, return_sequences=self.return_sequence , return_state=self.return_state)\n",
    "#         self.bidirectional = tf.keras.layers.Bidirectional(self.gru)\n",
    "        self.dense = tf.keras.layers.Dense(units=self.units , input_shape = (self.embedding_dim,))\n",
    "    \n",
    "    def call(self,x,headline):\n",
    "#         x = self.embedding(x)\n",
    "#         headline = self.embedding(headline)\n",
    "        headline = tf.reduce_mean(headline,axis=1)\n",
    "        headline  = self.dense(headline)\n",
    "        output , state = self.gru(x,initial_state=headline)\n",
    "        return output , state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 460, 16)\n",
      "(1, 16)\n"
     ]
    }
   ],
   "source": [
    "output , state = encoder(d_list , h_list)\n",
    "print(output.shape)\n",
    "print(state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 16)\n",
      "(1, 460, 1)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(state, output)\n",
    "print(attention_result.shape)\n",
    "print(attention_weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, units = 16,activation='tanh',recurrent_activation='sigmoid',return_state=True,return_sequence=False , vocab_size=40000 , embedding_dim=30 , batch_size=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.return_state = return_state\n",
    "        self.return_sequence = return_sequence\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "#         self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "#                                        return_sequences=True,\n",
    "#                                        return_state=True,\n",
    "#                                        recurrent_initializer='glorot_uniform')\n",
    "        self.gru = tf.keras.layers.GRU(units=self.units , activation=self.activation , recurrent_activation=self.activation, return_sequences=self.return_sequence , return_state=self.return_state)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.units)\n",
    "\n",
    "    def call(self, x, hidden, output):\n",
    "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, output)\n",
    "\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "#         x = self.embedding(x)\n",
    "\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[1]))\n",
    "\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder output shape: (batch_size, vocab size) (1, 40000)\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder()\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((1, 1,50)),state,output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(text,headline):\n",
    "    inputs = tf.convert_to_tensor(text)\n",
    "    h = tf.convert_to_tensor(headline)\n",
    "    result = ''\n",
    "\n",
    "    enc_out, enc_hidden = encoder(inputs, h)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims(tf.expand_dims(tf.convert_to_tensor(weights[word2idx.index('start')]), 0),0)\n",
    "    for t in range(100):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        result += word2idx[predicted_id] + ' '\n",
    "\n",
    "        if word2idx[predicted_id] == '<end>':\n",
    "            return result, sentence, attention_plot\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims(tf.expand_dims(tf.convert_to_tensor(weights[predicted_id]), 0),0)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(text,headline):\n",
    "    result = evaluate(text,headline)\n",
    "    print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted translation: mclean reflective rouse 51st vestiges ascot secessionist dir wildly chagrin haider affirmed 7000 iranian afghanistan schott gainer sarin redress hysterical hercegovina slovene confessing wahl quotations painless zoologist nationalist fatality cra dur sensual fredrik erodes coughing lived outsiders trailed farrar minimized seifert rafting hp 10:30 lak midsize 7000 iranian afghanistan schott gainer sarin redress hysterical hercegovina slovene confessing wahl quotations painless zoologist nationalist fatality cra dur sensual fredrik erodes coughing lived outsiders trailed farrar minimized seifert rafting hp 10:30 lak midsize 7000 iranian afghanistan schott gainer sarin redress hysterical hercegovina slovene confessing wahl quotations painless zoologist nationalist fatality cra dur sensual \n"
     ]
    }
   ],
   "source": [
    "translate(d_list,h_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 457, 50)\n"
     ]
    }
   ],
   "source": [
    "#document Embedding\n",
    "features = {}\n",
    "d_list=[]\n",
    "features['word_indices'] = nltk.word_tokenize(x[5]) #embedding on the document\n",
    "# features['word_indices'] = [weights[word2idx.index(word)] for word in features['word_indices']]\n",
    "for word in features['word_indices']:\n",
    "    if word in word2idx:\n",
    "        features['word_indices'] = [weights[word2idx.index(word)]]\n",
    "    else:\n",
    "        features['word_indices'] =[weights[word2idx.index('UNK')]]\n",
    "    d_list.append(features['word_indices'])\n",
    "\n",
    "d_list = tf.convert_to_tensor(d_list)\n",
    "d_list = tf.reshape(d_list,(1,d_list.shape[0],d_list.shape[2]))\n",
    "print(d_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 18, 50)\n"
     ]
    }
   ],
   "source": [
    "# Headline Embedding\n",
    "h_features ={}\n",
    "h_list=[]\n",
    "h_features['headline_word_indices'] = nltk.word_tokenize(headline[5])\n",
    "for word in h_features['headline_word_indices']:\n",
    "    if word in word2idx:\n",
    "        h_features['headline_word_indices']=weights[word2idx.index(word)]\n",
    "    else:\n",
    "        h_features['headline_word_indices']=weights[word2idx.index('UNK')] \n",
    "    h_list.append(h_features['headline_word_indices'])\n",
    "\n",
    "h_list = tf.convert_to_tensor(h_list)\n",
    "h_list = tf.reshape(h_list,(-1,h_list.shape[0],h_list.shape[1]))\n",
    "print(h_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted translation: stowe hardest zoologist nationalist fatality cra dur sensual fredrik rm1 esther bauer 2005-2006 struggles fiore showed xfdws 1941 540 renovated sauce imaginative 29 soloists attila cognitive veered co-written skye pvt. bona asad scandal madre mart blended hilary petroleum fermentation coyne ezra painless tammy cnpc pies metacritic bailing riot 47-42-80-44 díaz fascists salespeople bestselling shortly criticizes recharge illawarra connor mounted existing bronfman violent appointments olivetti violent getter françois phrases acquisitions holmgren devise tribesmen competitors five-year combo customized folks mohan tethered neverland wahn ... handheld harp gainer sarin redress qualms computed reiterate microscope luzon positively cline nears stabilisation adviser gail 500m organises \n"
     ]
    }
   ],
   "source": [
    "translate(d_list,h_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
