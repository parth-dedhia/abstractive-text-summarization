{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>headlines</th>\n",
       "      <th>read_more</th>\n",
       "      <th>text</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chhavi Tyagi</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>http://www.hindustantimes.com/india-news/raksh...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Daisy Mowke</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>http://www.hindustantimes.com/bollywood/malaik...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Arshiya Chopra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Virgin' now corrected to 'Unmarried' in IGIMS'...</td>\n",
       "      <td>http://www.hindustantimes.com/patna/bihar-igim...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sumedha Sehra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/abu-dujana-...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aarushi Maheshwari</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/sex-traffic...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               author                  date  \\\n",
       "0        Chhavi Tyagi  03 Aug 2017,Thursday   \n",
       "1         Daisy Mowke  03 Aug 2017,Thursday   \n",
       "2      Arshiya Chopra  03 Aug 2017,Thursday   \n",
       "3       Sumedha Sehra  03 Aug 2017,Thursday   \n",
       "4  Aarushi Maheshwari  03 Aug 2017,Thursday   \n",
       "\n",
       "                                           headlines  \\\n",
       "0  Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1  Malaika slams user who trolled her for 'divorc...   \n",
       "2  Virgin' now corrected to 'Unmarried' in IGIMS'...   \n",
       "3  Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4  Hotel staff to get training to spot signs of s...   \n",
       "\n",
       "                                           read_more  \\\n",
       "0  http://www.hindustantimes.com/india-news/raksh...   \n",
       "1  http://www.hindustantimes.com/bollywood/malaik...   \n",
       "2  http://www.hindustantimes.com/patna/bihar-igim...   \n",
       "3  http://indiatoday.intoday.in/story/abu-dujana-...   \n",
       "4  http://indiatoday.intoday.in/story/sex-traffic...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "\n",
       "                                               ctext  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('news_summary.csv')#,encoding=\"ISO-8859-1\")\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna(how='any',axis=0) \n",
    "x = dataset['ctext']\n",
    "y = dataset['text']\n",
    "headline = dataset['headlines']\n",
    "del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(w):\n",
    "    w = w.lower().strip()\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.rstrip().strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "  \n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape : (4395, 5791) Target Tensor Shape : (4395, 86)\n",
      "Preprocessed Text : \n",
      " <start> the food safety and standards authority of india fssai is in the process of creating a network of food banking partners to collect and distribute leftover food from large parties and weddings to the hungry . a notification to create a separate category of food business operators fbos , who will be licensed to deal only with leftover food , has been drafted to ensure the quality of food . ? we are looking at partnering with ngos or organisations that collect , store and distribute surplus food to ensure they maintain certain hygiene and health standards when handling food , ? said pawan agarwal , ceo of fssai . ? tonnes of food is wasted annually . we are looking at creating a mechanism through which food can be collected from restaurants , weddings , large scale parties , ? says pawan agarwal , ? all food , whether it is paid for or distributed free , must meet the country ? s food safety and hygiene standards , ? he said . the organisations in the business of collecting leftover food will now have to work in collaboration with fssai so their efforts can be scaled up . ? tonnes of food is wasted annually and can be used to feed several thousands . we are looking at creating a mechanism through which food can be collected from restaurants , weddings , large scale parties etc , ? said agarwal . the initiative will set up a helpline network where organisations can call in for collection but reaching individuals who want to directly donate food will take time . ? we will have a central helpline number . reaching people at the household level may not be feasible initially but it is an integral part of the long term plan , ? he said . ? we have begun collecting names of people working in the sector . there are still a few months to go before the scheme materialises , ? said agarwal . ? collecting food going waste to feed the hungry is a noble thought but to transport , store and maintain the cold chain of cooked food is a huge challenge . the logistics are a nightmare , which is why we don ? t handle leftovers and only distribute uncooked food that can be cooked locally , ? said kuldip nar , founder of delhi ncr food bank , which has been feeding the poor in cities since . <end>\n",
      "Tokenized Text : \n",
      " [ 42   1 599 ...   0   0   0]\n",
      "Headline Text : \n",
      " <start> food regulator planning leftover banks to feed hungry people <end>\n",
      "Headline Tokenized : \n",
      " [   1  397 2727  798 4107  424    3 4108 2728   70    2    0    0    0\n",
      "    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "x = [preprocess_sentence(w) for w in x]\n",
    "y = [preprocess_sentence(w) for w in y]\n",
    "headline = [preprocess_sentence(w) for w in headline]\n",
    "x_train, inp_lang = tokenize(x)\n",
    "y_train, targ_lang = tokenize(y)\n",
    "headline_train , headline_lang = tokenize(headline)\n",
    "print(\"Input Tensor Shape :\" , x_train.shape , \"Target Tensor Shape :\" , y_train.shape)\n",
    "print(\"Preprocessed Text : \\n\" , x[10])\n",
    "print(\"Tokenized Text : \\n\" , x_train[10])\n",
    "print(\"Headline Text : \\n\" , headline[10])\n",
    "print(\"Headline Tokenized : \\n\" , headline_train[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    '''\n",
    "    The code of encoder is used for text summarization. The model needs an initial_state which is the headline of the article.\n",
    "    The input is supposed to be padded sequence for initial state as well as for the input. The model cuurently implements\n",
    "    GRU for the encoder sequence as there is some bug with using lstm with bidirectional. Bidirectional was also tried with \n",
    "    GRU but same bug occured.\n",
    "    \n",
    "    The initialization is based on docuemnt context vector paper referenced from \n",
    "    https://arxiv.org/pdf/1807.08000.pdf paper named \n",
    "    Abstractive and Extractive Text Summarization using DocumentContext Vector and Recurrent Neural Networks.\n",
    "    \n",
    "    However the logic is modeified. The model takes the preprocessed tokenized padded sequence as input for initials state\n",
    "    and the then it is passed through the embedded vector and the size of the output vector after embedding is (batch_size , time , embedding size)\n",
    "    after this the model is averaged about the time axis and gives the output as (batch_size , embedding size).\n",
    "    This tensor is then passed as input to dence vector and is used to give the output size of (batch_zie , lgur_hidden_units)\n",
    "    '''\n",
    "    def __init__(self,units = 16,activation='tanh',recurrent_activation='sigmoid',return_state=True,return_sequence=True , vocab_size=30000 , embedding_dim=30 , batch_size=2):\n",
    "        super (Encoder , self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.recurrent_activation = recurrent_activation\n",
    "        self.return_state = return_state\n",
    "        self.return_sequence = return_sequence\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "#         self.lstm = tf.keras.layers.LSTM(units=self.units , activation=self.activation , recurrent_activation=self.activation, return_sequences=self.return_sequence , return_state=self.return_state)\n",
    "        self.gru = tf.keras.layers.GRU(units=self.units , activation=self.activation , recurrent_activation=self.activation, return_sequences=self.return_sequence , return_state=self.return_state)\n",
    "#         self.bidirectional = tf.keras.layers.Bidirectional(self.gru)\n",
    "        self.dense = tf.keras.layers.Dense(units=self.units , input_shape = (self.embedding_dim,))\n",
    "    \n",
    "    def call(self,x,headline):\n",
    "        x = self.embedding(x)\n",
    "        print(x.shape)\n",
    "        headline = self.embedding(headline)\n",
    "        headline = tf.reduce_mean(headline,axis=1)\n",
    "        headline  = self.dense(headline)\n",
    "        output , state = self.gru(x,initial_state=headline)\n",
    "        return output , state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5791, 30)\n",
      "tf.Tensor(\n",
      "[[[ 0.03370849  0.01697699  0.02288595 ...  0.0255639   0.0256319\n",
      "   -0.03076383]\n",
      "  [ 0.0209395  -0.01639135 -0.0198533  ...  0.01310598  0.03307112\n",
      "    0.02388732]\n",
      "  [-0.00479571 -0.02213357 -0.05248106 ... -0.01754641  0.02041301\n",
      "   -0.00901647]\n",
      "  ...\n",
      "  [ 0.03356769 -0.02665575  0.05065165 ... -0.02117584 -0.02182933\n",
      "    0.00680007]\n",
      "  [ 0.03356769 -0.02665575  0.05065165 ... -0.02117584 -0.02182933\n",
      "    0.00680007]\n",
      "  [ 0.03356769 -0.02665575  0.05065165 ... -0.02117584 -0.02182933\n",
      "    0.00680007]]\n",
      "\n",
      " [[ 0.0329232   0.01725838  0.02296641 ...  0.02570033  0.02584546\n",
      "   -0.03112317]\n",
      "  [-0.00516574 -0.03667758 -0.01311471 ... -0.04633377  0.02854744\n",
      "    0.04107402]\n",
      "  [ 0.04356331 -0.02771307  0.01416744 ...  0.02028258 -0.012892\n",
      "   -0.0253625 ]\n",
      "  ...\n",
      "  [ 0.03356769 -0.02665575  0.05065165 ... -0.02117584 -0.02182933\n",
      "    0.00680007]\n",
      "  [ 0.03356769 -0.02665575  0.05065165 ... -0.02117584 -0.02182933\n",
      "    0.00680007]\n",
      "  [ 0.03356769 -0.02665575  0.05065165 ... -0.02117584 -0.02182933\n",
      "    0.00680007]]], shape=(2, 5791, 16), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.03356769 -0.02665575  0.05065165  0.01291954 -0.03526251 -0.00170908\n",
      "   0.00581601 -0.03308893 -0.02425826  0.00043227 -0.01553621 -0.01549156\n",
      "  -0.04209173 -0.02117584 -0.02182933  0.00680007]\n",
      " [ 0.03356769 -0.02665575  0.05065165  0.01291954 -0.03526251 -0.00170908\n",
      "   0.00581601 -0.03308893 -0.02425826  0.00043227 -0.01553621 -0.01549156\n",
      "  -0.04209173 -0.02117584 -0.02182933  0.00680007]], shape=(2, 16), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "output , state = encoder(x_train[:2] , headline_train[:2])\n",
    "print(output)\n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # we are doing this to perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.03167229 -0.02504718  0.04648036  0.01211604 -0.03271927 -0.00174045\n",
      "   0.00540717 -0.03070412 -0.02240552  0.00062074 -0.01431609 -0.01458084\n",
      "  -0.03872574 -0.01939478 -0.02021422  0.00645646]\n",
      " [ 0.03126151 -0.02471879  0.046278    0.01193519 -0.03219241 -0.00193332\n",
      "   0.00544128 -0.03033368 -0.02198728  0.00046814 -0.01438568 -0.01461027\n",
      "  -0.03858323 -0.01931183 -0.02045809  0.00633706]], shape=(2, 16), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[0.00016393]\n",
      "  [0.00016412]\n",
      "  [0.00016388]\n",
      "  ...\n",
      "  [0.00017305]\n",
      "  [0.00017305]\n",
      "  [0.00017305]]\n",
      "\n",
      " [[0.00016392]\n",
      "  [0.0001546 ]\n",
      "  [0.0001723 ]\n",
      "  ...\n",
      "  [0.00017301]\n",
      "  [0.00017301]\n",
      "  [0.00017301]]], shape=(2, 5791, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(state, output)\n",
    "print(attention_result)\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
